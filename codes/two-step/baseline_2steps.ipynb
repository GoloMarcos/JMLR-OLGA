{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!gdown 1cP2VcLXJnj1ntyg26UNPYD9NzYb__iRS\n",
        "!unzip preprocessed.zip\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "basepath = Path('preprocessed/')\n",
        "files_in_basepath = basepath.iterdir()\n",
        "datasets = {}\n",
        "for item in files_in_basepath:\n",
        "  if item.is_file():\n",
        "    df = pd.read_pickle('preprocessed/' + item.name)\n",
        "    datasets[item.name.replace('.pkl','')] = df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-bsUHGoJ0KP",
        "outputId": "a945add1-c20f-4099-f363-118de38077b7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1cP2VcLXJnj1ntyg26UNPYD9NzYb__iRS\n",
            "To: /content/preprocessed.zip\n",
            "100% 57.7M/57.7M [00:02<00:00, 26.0MB/s]\n",
            "Archive:  preprocessed.zip\n",
            "   creating: preprocessed/\n",
            "  inflating: preprocessed/fakenews.pkl  \n",
            "  inflating: preprocessed/strawberry.pkl  \n",
            "  inflating: preprocessed/relevant_reviews.pkl  \n",
            "  inflating: preprocessed/terrorism.pkl  \n",
            "  inflating: preprocessed/food.pkl   \n",
            "  inflating: preprocessed/musk.pkl   \n",
            "  inflating: preprocessed/pneumonia.pkl  \n",
            "  inflating: preprocessed/TUANDROMD.pkl  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse.lil import lil_matrix\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "import networkx as nx\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def train_test_split_OCL(df, seed=81, folds=10):\n",
        "\n",
        "  kf = KFold(n_splits=folds, shuffle=True, random_state=seed)\n",
        "\n",
        "  df_int = df[df.is_interest == 1]\n",
        "  df_nint = df[df.is_interest == 0]\n",
        "\n",
        "  l_index_int = []\n",
        "\n",
        "  for train_index, test_index in kf.split(df_int):\n",
        "    df_train = df_int.iloc[train_index]\n",
        "    df_test = df_int.iloc[test_index]\n",
        "\n",
        "    df_int_train, df_int_val = train_test_split(df_train, test_size=0.1, random_state=seed)\n",
        "\n",
        "    l_index_int.append([df_int_train.index, df_int_val.index, df_test.index])\n",
        "\n",
        "  l_index_nint = []\n",
        "  for i in range(folds):\n",
        "    df_nint_val, df_nint_test = train_test_split(df_nint, test_size=0.5, random_state=i)\n",
        "    l_index_nint.append([df_nint_val.index, df_nint_test.index])\n",
        "\n",
        "  return l_index_int, l_index_nint\n",
        "\n",
        "def generate_graph(df,k,metric, folds=10):\n",
        "\n",
        "  G = kneighbors_graph(df['features'].to_list(), k, mode='connectivity', include_self=False, metric=metric)\n",
        "\n",
        "  graph_networkx = nx.Graph(G)\n",
        "\n",
        "  for i,row in df.iterrows():\n",
        "    graph_networkx.nodes[i]['features'] = row['features']\n",
        "    graph_networkx.nodes[i]['label'] = row['is_interest']\n",
        "\n",
        "  return graph_networkx"
      ],
      "metadata": {
        "id": "2r5Ph-IiJ104",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d7ff87a-92fc-433d-c23f-35a4102118d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-f8b5b6e4b048>:1: DeprecationWarning: Please use `lil_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.lil` namespace is deprecated.\n",
            "  from scipy.sparse.lil import lil_matrix\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5E7MUhAEy-U"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import random\n",
        "import numpy as np\n",
        "from typing import List\n",
        "from tqdm import tqdm\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import argparse\n",
        "import os.path as osp\n",
        "\n",
        "\n",
        "class DeepWalk:\n",
        "    def __init__(self, window_size: int, embedding_size: int, walk_length: int, walks_per_node: int):\n",
        "        \"\"\"\n",
        "        :param window_size: window size for the Word2Vec model\n",
        "        :param embedding_size: size of the final embedding\n",
        "        :param walk_length: length of the walk\n",
        "        :param walks_per_node: number of walks per node\n",
        "        \"\"\"\n",
        "        self.window_size = window_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.walk_length = walk_length\n",
        "        self.walk_per_node = walks_per_node\n",
        "\n",
        "    def random_walk(self, g: nx.Graph, start: str, use_probabilities: bool = False) -> List[str]:\n",
        "        \"\"\"\n",
        "        Generate a random walk starting on start\n",
        "        :param g: Graph\n",
        "        :param start: starting node for the random walk\n",
        "        :param use_probabilities: if True take into account the weights assigned to each edge to select the next candidate\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        np.random.seed(81)\n",
        "        random.seed(81)\n",
        "        walk = [start]\n",
        "        for i in range(self.walk_length):\n",
        "            neighbours = g.neighbors(walk[i])\n",
        "            neighs = list(neighbours)\n",
        "            if use_probabilities:\n",
        "                probabilities = [g.get_edge_data(walk[i], neig)[\"weight\"] for neig in neighs]\n",
        "                sum_probabilities = sum(probabilities)\n",
        "                probabilities = list(map(lambda t: t / sum_probabilities, probabilities))\n",
        "                p = np.random.choice(neighs, p=probabilities)\n",
        "            else:\n",
        "                p = random.choice(neighs)\n",
        "            walk.append(p)\n",
        "\n",
        "        return [str(w) for w in walk]\n",
        "\n",
        "    def get_walks(self, g: nx.Graph, use_probabilities: bool = False) -> List[List[str]]:\n",
        "        \"\"\"\n",
        "        Generate all the random walks\n",
        "        :param g: Graph\n",
        "        :param use_probabilities:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        np.random.seed(81)\n",
        "        random.seed(81)\n",
        "        random_walks = []\n",
        "        for _ in range(self.walk_per_node):\n",
        "            random_nodes = list(g.nodes)\n",
        "            random.shuffle(random_nodes)\n",
        "            for node in random_nodes:\n",
        "                random_walks.append(self.random_walk(g=g, start=node, use_probabilities=use_probabilities))\n",
        "        return random_walks\n",
        "\n",
        "    def compute_embeddings(self, walks: List[List[str]]):\n",
        "        \"\"\"\n",
        "        Compute the node embeddings for the generated walks\n",
        "        :param walks: List of walks\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        model = Word2Vec(sentences=walks, window=self.window_size, size=self.embedding_size)\n",
        "        return model.wv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.12.0+cu113.html"
      ],
      "metadata": {
        "id": "_ukD1mkA6SCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch_geometric.utils as utils\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.nn import GAE, GCNConv\n",
        "\n",
        "class GCNEncoder(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 2 * out_channels)\n",
        "        self.conv2 = GCNConv(2 * out_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        return self.conv2(x, edge_index)\n",
        "\n",
        "\n",
        "def train(model, optimizer, dataset):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    z = model.encode(dataset.features.float(), dataset.edge_index)\n",
        "    loss = model.recon_loss(z, dataset.edge_index)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return float(loss)\n",
        "\n",
        "def gae_train(g):\n",
        "\n",
        "  random.seed(81)\n",
        "  np.random.seed(81)\n",
        "  torch.manual_seed(81)\n",
        "  torch.cuda.manual_seed_all(81)\n",
        "\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "  dataset = utils.from_networkx(g)\n",
        "\n",
        "  in_channels, out_channels = len(dataset.features[0]), 2\n",
        "\n",
        "  model = GAE(GCNEncoder(in_channels, out_channels))\n",
        "\n",
        "  model = model.to(device)\n",
        "  model = model.float()\n",
        "\n",
        "  dataset = dataset.to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "  for epoch in range(100):\n",
        "      loss = train(model, optimizer, dataset)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    embs = model.encode(dataset.features.float(), dataset.edge_index)\n",
        "    return embs.cpu().numpy()"
      ],
      "metadata": {
        "id": "P5Ah-mP2wjyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install node2vec"
      ],
      "metadata": {
        "id": "QLppvko1v9kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from node2vec import Node2Vec\n",
        "\n",
        "def node_to_vec(g):\n",
        "\n",
        "  np.random.seed(81)\n",
        "  random.seed(81)\n",
        "  model = Node2Vec(g, dimensions=2, walk_length=10, num_walks=15, q=1, p=4)\n",
        "\n",
        "  model = model.fit(window=10, min_count=1, batch_words=4)\n",
        "\n",
        "  for node in g.nodes():\n",
        "    g.nodes[node]['features_node2vec_2'] = model[str(node)]\n",
        "\n",
        "def deepwalk(g):\n",
        "\n",
        "  random.seed(81)\n",
        "  np.random.seed(81)\n",
        "  dw = DeepWalk(window_size = 10, embedding_size = 2, walk_length = 10, walks_per_node = 15)\n",
        "\n",
        "  walks = dw.get_walks(g)\n",
        "\n",
        "  emb = dw.compute_embeddings(walks).vectors\n",
        "\n",
        "  for node in g.nodes():\n",
        "    g.nodes[node]['features_deepwalk_2'] = emb[node]\n",
        "\n",
        "def gae(g):\n",
        "  embs = gae_train(g)\n",
        "\n",
        "  for node in g.nodes():\n",
        "    g.nodes[node]['features_gae_2'] = embs[node]"
      ],
      "metadata": {
        "id": "mKeTYfwtn7Xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for dataset in datasets.keys():\n",
        "  print(dataset)\n",
        "  for k in [1,2,3]:\n",
        "    print(k)\n",
        "\n",
        "    df = datasets[dataset]\n",
        "\n",
        "    g = generate_graph(df,k,'euclidean')\n",
        "\n",
        "    deepwalk(g)\n",
        "    node_to_vec(g)\n",
        "    gae(g)\n",
        "\n",
        "    l_index_int, l_index_nint = train_test_split_OCL(df, folds=10)\n",
        "\n",
        "    for f in range(10):\n",
        "      for i in g.nodes():\n",
        "        if i in l_index_int[f][0]:\n",
        "          g.nodes[i]['train'] = 1\n",
        "          g.nodes[i]['val'] = 0\n",
        "          g.nodes[i]['test'] = 0\n",
        "        elif i in l_index_int[f][1] or i in l_index_nint[f][0]:\n",
        "          g.nodes[i]['train'] = 0\n",
        "          g.nodes[i]['val'] = 1\n",
        "          g.nodes[i]['test'] = 0\n",
        "        elif i in l_index_int[f][2] or i in l_index_nint[f][1]:\n",
        "          g.nodes[i]['train'] = 0\n",
        "          g.nodes[i]['val'] = 0\n",
        "          g.nodes[i]['test'] = 1\n",
        "\n",
        "      path = '/content/drive/MyDrive/USP/Doctorate/Research/Articles/Auto One-Class Graph Neural Network/datasets/graphs/' + dataset + '/k=' + str(k) + '/'\n",
        "      name = dataset + '_k=' + str(k) + '_fold=' + str(f) + '.gpickle'\n",
        "      nx.write_gpickle(g, path+name)"
      ],
      "metadata": {
        "id": "dwuj9JtINi8K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}